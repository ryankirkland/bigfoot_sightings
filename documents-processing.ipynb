{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools, technologies, & techniques featured in this notebook\n",
    "- NLP preprocessing, clustering, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import svd\n",
    "# import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text ingestion\n",
    "**Source selection**\n",
    "- Observation data was imported from the file you provided. We looked for other sources, but found that the publishers of this information took the most care to be most credible. A team of researchers would follow up on each sighting with an interview and collection of 'evidence' and attempt to consistently classify the report. They only publish the top three tiers of credibility--A through C in order of most to least evidence.\n",
    "\n",
    "**Data import and data wrangling**\n",
    "\n",
    "- 'Beautiful Soup' module to get the html data into a usable format\n",
    "- Data was pretty messy--think of looking through a filing cabinet for a document where the person who was in charge of filing didn't reliably put files in the right folders\n",
    "- Straightened out the filing cabinet and then obtained year and month, state and county, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/salvir1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "# porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer('english')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punc(string:str) -> str:\n",
    "    '''Given a string, removes all punctuation and returned punctuation-less string'''\n",
    "    return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(str):\n",
    "    '''\n",
    "    Tokenize a str and return a tokenized list.\n",
    "    '''\n",
    "    return [word for word in word_tokenize(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    '''Takes in a doc and lemmatizes tokens in doc\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized tokens\n",
    "    '''\n",
    "    return [wordnet.lemmatize(tkn) for tkn in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_stop_words(doc, stops=set(stopwords.words('english'))):\n",
    "    '''Takes in a doc and removes stop words\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokens with stop words removed\n",
    "    '''\n",
    "    return([w for w in doc if w not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(content):\n",
    "    '''\n",
    "    Add docstring. Make flexible to allow for doing, or not doing, preprocessing functions. \n",
    "    Parameters\n",
    "    ----------\n",
    "    content (str): a collection of strings\n",
    "    Returns\n",
    "    -------\n",
    "    A list of lists: each list contains a tokenized version of the original string\n",
    "    '''\n",
    "    preprocessed = []\n",
    "    for i in range(len(content)):\n",
    "        step_1 = remove_punc(content[i].lower())\n",
    "        step_2 = tokenize(step_1)\n",
    "        step_3 = lemmatize(step_2)\n",
    "        step_4 = rm_stop_words(step_3)\n",
    "        preprocessed.append(step_4)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading bigfoot data\n",
    "sightings_df = pd.read_csv('test_observations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>observations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OBSERVED: I and two of my friends were bored o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>OBSERVED: To whom it may concern, I am a comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>OBSERVED: It was the month of July, 2009 in Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>OBSERVED: From the top of this hill the river ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>OBSERVED: It was a hot afternoon in august. I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       observations\n",
       "0           0  OBSERVED: I and two of my friends were bored o...\n",
       "1           1  OBSERVED: To whom it may concern, I am a comme...\n",
       "2           2  OBSERVED: It was the month of July, 2009 in Fa...\n",
       "3           3  OBSERVED: From the top of this hill the river ...\n",
       "4           4  OBSERVED: It was a hot afternoon in august. I ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sightings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading sample data to check functions\n",
    "# articles = pd.read_pickle('data/articles.pkl')\n",
    "# articles.head(1)\n",
    "\n",
    "# corpus = articles['content']\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing--data load and function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokenized = preprocess_corpus(sightings_df['observations']) # cleaned and tokenized\n",
    "\n",
    "for i in range(len(cleaned_tokenized)): # for the bigfoot observations column only, remove first occurrence of observed\n",
    "    del(cleaned_tokenized[i][0])\n",
    "\n",
    "str_cleaned_tokenized = [\" \".join(x) for x in cleaned_tokenized] # string version of cleaned and tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Bag of words function'\n",
    "vect = CountVectorizer(max_features=500)\n",
    "word_counts = vect.fit_transform(str_cleaned_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12316568, 0.07417744, 0.06920658, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.06732157, 0.12163474, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.20386531,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(max_features=500)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(str_cleaned_tokenized)\n",
    "tfidf_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 5\n",
    "kmeans = KMeans(n_clusters=clusters, \n",
    "                random_state=0).fit(tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Investigate the clusters  \n",
    "\n",
    "> - Investigate the 'centroids' to find out what \"topics\" Kmeans has discovered by mapping these vectors back into the 'word space'.  Think of each feature/dimension of the centroid vector as representing the \"average\" article or the average occurrences of words for that cluster.\n",
    "   \n",
    "> - Find the features/dimensions with the greatest representation in the centroid.  Print out the top ten words for each centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "river wa friend away heard walking stream large moving looked hill hairy creature fishing eye mother like noise tree saw brush could foot get \n",
      "\n",
      "sound heard wa recording howl howling like bigfoot wife coyote park night anything witness report area ive never thought site around hear would life \n",
      "\n",
      "wa road back saw like wood time foot something see tree around tall house would came could got dog one creature heard friend never \n",
      "\n",
      "creature wa crossed road animal saw car driving hill fur slide highway side foot seen front brown brother home two leg bear encounter said \n",
      "\n",
      "wa lake heard scream time know one track im back year loud bigfoot like howl bear day area saw location thing could think family \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Sort(sub_li): \n",
    "    return sorted(sub_li, key = lambda x: x[0], reverse=True)\n",
    "\n",
    "def get_word(centroid):\n",
    "    return [x[1] for x in centroid]\n",
    "\n",
    "for k in range(5):\n",
    "    matched = zip(kmeans.cluster_centers_[k], tfidfvect.get_feature_names())\n",
    "    match = Sort(list(matched))\n",
    "    print(' '.join(get_word(match[:24])), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heirarchical clustering methods, see 819 am clustering assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "- Unsupervised learning\n",
    "\n",
    "- Use the cosine similarity to compare similarity between documents.\n",
    "\n",
    "- sklearn's [linear_kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html) (computes dot product) can be used on tfidf to compute the cosine similarity since rows are normalized.*\n",
    "\n",
    "- Here's a page on cosine similarity from [sklearn documentation](http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) and a relevant [stack overflow post](http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity).\n",
    "\n",
    "- *The stack overflow post is helpful. It provides instruction over how to slice the tfidf and then how to apply cosine similarity between one doc and all of the rest.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.13519924, 0.10140883, 0.13256265, 0.09973631,\n",
       "       0.08332324, 0.13919403, 0.04549516, 0.11405611, 0.03008794,\n",
       "       0.15609695, 0.13999869, 0.10323763, 0.05055173, 0.10641081,\n",
       "       0.18633001, 0.15567446, 0.08368858, 0.07955329, 0.08235686,\n",
       "       0.16156469, 0.07326499, 0.07967395, 0.18867646, 0.20768301,\n",
       "       0.11664398, 0.17716142, 0.10061139, 0.05556414, 0.05760488,\n",
       "       0.04071619, 0.05235034, 0.09572017, 0.1483815 , 0.17424577,\n",
       "       0.08761548, 0.13279674, 0.16303554, 0.15010067, 0.03747498,\n",
       "       0.04447924, 0.08086749, 0.13108412, 0.03843005, 0.1572968 ,\n",
       "       0.0620882 , 0.05965807, 0.08508997, 0.10648865, 0.12334789,\n",
       "       0.17949813, 0.1091755 , 0.04655598, 0.10652089, 0.12321672,\n",
       "       0.20605397, 0.06152691, 0.12759122, 0.14451207, 0.18906863,\n",
       "       0.03578539, 0.10891749, 0.08089774, 0.06584412, 0.20383462,\n",
       "       0.12232553, 0.16047753, 0.08208069, 0.11080879, 0.07065213,\n",
       "       0.1003166 , 0.23102123, 0.09029041, 0.13398229, 0.12873122,\n",
       "       0.10401529, 0.07803374, 0.26692321, 0.12805912, 0.33967007,\n",
       "       0.13467093, 0.09042538, 0.04217383, 0.21230989, 0.07474441,\n",
       "       0.12979568, 0.18845343, 0.0764166 , 0.03020119, 0.17516674,\n",
       "       0.16690025, 0.25204317, 0.17733897, 0.24058874, 0.2805783 ,\n",
       "       0.0924762 , 0.07981193, 0.22922236, 0.16598739, 0.16101498,\n",
       "       0.29222702, 0.09458308, 0.19372897, 0.07159852])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_vectorized[1:2], tfidf_vectorized[1:500]).flatten() # This is comparing cs for article #2 and the next 500.\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  79 100  94  77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.33967007, 0.29222702, 0.2805783 , 0.26692321])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-6:-1] # This identifies the index of the top 5 most similar.\n",
    "print(related_docs_indices)\n",
    "\n",
    "most_similar = cosine_similarities[related_docs_indices] # and their related cs\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBSERVED: I and two of my friends were bored one night so we decided to do a little snowmachining. Though it was illegal to snowmachine in Anchorage, there were some good trails to ride on a little north of my house.  We took off at probably 11 pm, rode up the road about a quarter mile, and cut off on the trails. It had snowed about 10 inches a few days before so there was fresh snow, with no tracks.  I was leading the way for about a half hour, then we stopped and talked for a little bit.  We took off again and kept cruising on some sort of game trail that led to an opening in the woods.  I rode off into the opening with my friends following about fifty yards behind me.  I came over this little mound and saw strange tracks leading to this spot in the snow where it looked like something had pushed aside some snow and layed down.  I figured it was just a moose or something.  But I followed the tracks over the next small hill and as I came down the far side my headlight pointed right on the back of a bigfoot.  It was only about 10 to 12 feet in front of me.  It was running in the opposite direction.  I slammed on the brakes because I was scared out of my mind.  It continued to run away, jumped over a dead log covered in snow and disappeared into a group of trees and into the darkness.  I was so surprised and scared I quickly turned around and rode back toward my friends. I met them back by the first mound and said \"We need to get out of here,\"  and rode back towards my house.  When I told them about it back near my house, they laughed and told me it was probably a bear or someone in the woods.  But I was 100% positive that is was not a bear or anything else. The way it was running through the deep snow made me sure that it wasn't anything human.  For a long time I was made fun of and everyone told me I was crazy so I didn't like talking about it.  \n",
      "OBSERVED:  I was on the Thumb Butte Loop in Prescott next to the city park. While driving I spotted  a reddish- brown ape like creature. I stopped the car immediately and observed the creature. The hair on his arms was longer than the rest. It appeared to have little or no neck. It appeared to be very muscular and covered ground quickly. It was hard to judge the height of the creature because of the distance and shock but I knew it was tall. It walked in the tree line and there was a gap in the tree's about 75 meters long. I watched this creature walk through the opening. I remember his arms swinging and he only looked forward. I estimated (through military experience) that it was 100 meters from my position. I was sitting in the car with my head turned looking out the back and side window. I estimate that I saw him for at least 10 seconds although it seemed longer.  What I remember most is my mind was wrestling with the thought that what I was seeing was not supposed to exist yet I was viewing it. It was a most unsettling experience. What I tell you is the truth. I have never told anyone out of fear of ridicule and having my sanity questioned. I hope this information can help your organization.\n",
      "OBSERVED: During the last week of June, my girlfriend and I decided to take the dogs on a long walk. We were driving up the coast planning on a beach walk when I decided to go to a area on redwood creek where my friend had heard some unusual and scary whistles and screams. He is a very experienced hunter and outdoorsman with a degree in wildlife biology and he had never heard anything like it. Anyway, we were walking up the north bank, where the trails are, when I decided to cross to the south bank to check out some sandbars. I loaded up my girlfriend on my back and waded across. We then continued upstream a shortways when we came across three distinct footprints. They were approximately 14 1/2 inches long. I measured them with a stick that I saved. I compared them with my own barefoot track for depth and they were nearly twice as deep. I weigh about 280 lbs. I was surprised that they weren't wider, they were almost narrow for their length. I at first thought they were human because of this.  The best print had 5 very distinct toes and was flat, no arch at all, with toes straighter across than a normal human. It appeared that it had just leisurely walked into the creek. I scoured up and down for an exit point but the terrain wasn't suitable for tracks other than the odd sand bar. The strides were about 4 feet. It had come across a packed gravel bar leaving a few faint impressions. Nothing else observed.\n",
      "OBSERVED: It was late October 2006. My hunting partner and I were deer hunting near Heart Lake, which is located off the John Muir Trail behind Florence Lake.  We'd been back there for almost a week looking for the elusive Big Daddy Buck that we've hunted religiously for the last three years. We were high up on this ridge just before nightfall.  We were hiking back to our base camp and heard a loud bellowing howl that sent a shiver down our spines.  We looked at each other with confused looks on our faces, not knowing what that sound was.  We decided to hurry back to camp, but we heard another howl, but closer this time.  We saw Big Daddy Buck running down the opposite ridge side, and busting through brush, not caring if hunters spotted him.  Then again, we heard that bellowing howl, and saw on top of the ridge a dark silhouette that looked like a bear standing on its hind feet.  I brought my rifle scope to my eye to take aim on this so-called bear.  As I looked through the scope, I realized it was no bear.  I thought it could have been a really tall man in some kind of camouflage suit, but it looked like no man that I have ever seen.  It ran down the hill and disappeared into the brush, like it was chasing the deer.  So we decided to go and track down this creature.  Once we arrived at where it was standing, there was a footprint in the dirt that made my footprint look like a child's footprint.  It was four times the size of mine. We never saw the creature again and never heard that howl, but then again, I've never gone back since then.\n",
      "OBSERVED: i had just finished putting my children and wife to bed and had just stepped out the front door of my house to smoke a cigarette and put the cat out when i heard this very low but loud 'whooing' sound coming from the direction of the river bottoms. Hearing the sounds i distinctively remember from the sound recordings and this one was similar to the low, far away moan type sounds on the recording. To even hear the sound in real life made the hair on back of my neck stand up straight.  The sound only continued for about another minute and as soon as it silenced, all the dogs in the neighborhood, even coyotes in the distance, all at once began to bark and howl. I decided to stay inside that night and the next morning went to the tree line on top of the river to see or hear anything else, but nothing more than.\n"
     ]
    }
   ],
   "source": [
    "for i in related_docs_indices:\n",
    "    print(sightings_df['observations'].iloc[i]) # Going step by step pulling up the most similar reports by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions NMF (and SVD)\n",
    "- Unsupervised learning\n",
    "- Good for situations when there's some potentially valid grouping to both rows and columns, such as putting Joe and Sam in the same group because they like similar movies (as opposed to traditional supervised models where there are features and targets)\n",
    "- See 820pm solution to NMF for good soft classification and test of classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "- Supervised learning method to assign class probabilities to a document\n",
    "- See 818PM NLP-pipeline-programming-net-example for using sklearn Naive Bayes classifier. See also 818PM lecture on text classification. Solutions to assignment contain a number of useful naive Bayes python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('data/US_FIPS_Codes.csv', header=1)\n",
    "counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
